% EXAMPLE_CON_WU
% Script example pipeline Correlation CON WU
clear variables %#ok<*NASGU>
%% Load BrainAtlas
im_ba = ImporterBrainAtlasXLS( ...
    'FILE', [which('aal94_atlas.xlsx')], ...
    'WAITBAR', true ...
    );

ba = im_ba.get('BA');

%% load Nifty images
%%group1
im_gr1_WM_GM = ImporterGroupSubjNIfTI('DIRECTORY', [fileparts(which('AD_PositiveAmyloid.vois.xlsx')) filesep 'AD_PositiveAmyloid'], ...
    'NIFTI_TYPE', {'WM','GM'},...
    'WAITBAR', true);
gr1_WM_GM = im_gr1_WM_GM.get('GR');

im_gr1_PET = ImporterGroupSubjNIfTI('DIRECTORY', [fileparts(which('AD_PositiveAmyloid.vois.xlsx')) filesep 'AD_PositiveAmyloid'], ...
    'NIFTI_TYPE', {'wroriented_raw_pet'},...
    'WAITBAR', true);
gr1_PET = im_gr1_PET.get('GR');

%%group2
im_gr2_WM_GM = ImporterGroupSubjNIfTI('DIRECTORY',[fileparts(which('Healthy_NegativeAmyloid.vois.xlsx')) filesep 'Healthy_NegativeAmyloid'], ...
    'NIFTI_TYPE', {'WM','GM'},...
    'WAITBAR', true);
gr2_WM_GM = im_gr2_WM_GM.get('GR');

im_gr2_PET = ImporterGroupSubjNIfTI('DIRECTORY', [fileparts(which('Healthy_NegativeAmyloid.vois.xlsx')) filesep 'Healthy_NegativeAmyloid'], ...
    'NIFTI_TYPE', {'wroriented_raw_pet'},...
    'WAITBAR', true);
gr2_PET = im_gr2_PET.get('GR');

%%group3
im_gr3_WM_GM = ImporterGroupSubjNIfTI('DIRECTORY', [fileparts(which('MCI_PositiveAmyloid.vois.xlsx')) filesep 'MCI_PositiveAmyloid'], ...
    'NIFTI_TYPE', {'WM','GM'},...
    'WAITBAR', true);
gr3_WM_GM = im_gr3_WM_GM.get('GR');

im_gr3_PET = ImporterGroupSubjNIfTI('DIRECTORY', [fileparts(which('MCI_PositiveAmyloid.vois.xlsx')) filesep 'MCI_PositiveAmyloid'], ...
    'NIFTI_TYPE', {'wroriented_raw_pet'},...
    'WAITBAR', true);
gr3_PET = im_gr3_PET.get('GR');
%% PDF Construtor
path_dict = IndexedDictionary(...
    'IT_CLASS', 'FILE_PATH', ...
    'IT_LIST', {FILE_PATH('PATH', which('upsampled_AAL2.nii')),FILE_PATH('PATH', which('upsampled_TD.nii'))} ...
    );
% suvr_brain_label = readtable(which('AAL2_Atlas_Labels.csv'));
% suvr_brain_label = suvr_brain_label.Var4;
im_gr_pdf1 = PDFConstructor('GR_PET',gr1_PET, ...
    'GR_T1',gr1_WM_GM, ...
    'BA', ba,...
    'ATLAS_PATH_DICT' ,path_dict, ...
    'REF_REGION_LIST',{[9100,9110,9120,9130,9140,9150,9160,9170], 7}, ...
    'ATLAS_KIND', {'AAL2','TD'});
pdf_gr1 = im_gr_pdf1.get('GR');

im_gr_pdf2 = PDFConstructor('GR_PET',gr2_PET, ...
    'GR_T1',gr2_WM_GM, ...
    'BA', ba,...
    'ATLAS_PATH_DICT' ,path_dict, ...
    'REF_REGION_LIST',{[9100,9110,9120,9130,9140,9150,9160,9170], 7}, ...
    'ATLAS_KIND', {'AAL2','TD'});
pdf_gr2 = im_gr_pdf2.get('GR');

im_gr_pdf3 = PDFConstructor('GR_PET',gr3_PET, ...
    'GR_T1',gr3_WM_GM, ...
    'BA', ba,...
    'ATLAS_PATH_DICT' ,path_dict, ...
    'REF_REGION_LIST',{[9100,9110,9120,9130,9140,9150,9160,9170], 7}, ...
    'ATLAS_KIND', {'AAL2','TD'});
pdf_gr3 = im_gr_pdf3.get('GR');

%% Load Groups of SubjectCON Correlation based
g_temp  = GraphWU('STANDARDIZE_RULE', 'range');
a_WU1 = AnalyzeEnsemble_FUN_WU( ...
    'GR', pdf_gr1, ...
    'REPETITION', 0, ...
    'GRAPH_TEMPLATE', g_temp...
    );
gr1_corr = a_WU1.get('G_DICT');

a_WU2 = AnalyzeEnsemble_FUN_WU( ...
    'GR', pdf_gr2, ...
    'REPETITION', 0, ...
    'GRAPH_TEMPLATE', g_temp...
    );
gr2_corr = a_WU2.get('G_DICT');

a_WU3 = AnalyzeEnsemble_FUN_WU( ...
    'GR', pdf_gr3, ...
    'REPETITION', 0, ...
    'GRAPH_TEMPLATE', g_temp...
    );
gr3_corr = a_WU3.get('G_DICT');

%% NN DATASET
% ǵroup 1
[~, group_folder_name1] = fileparts(im_gr1_PET.get('DIRECTORY'));
it_list1 = cellfun(@(x) NNDataPoint_Graph_CLA( ...
    'ID', x.get('ID'), ...
    'G', x, ...
    'TARGET_CLASS', {group_folder_name1}), ...
     a_WU1.get('G_DICT').get('IT_LIST'), ...
    'UniformOutput', false);

% Get the subject dictionary and extract the list of subjects
sub_dict1 = gr1_WM_GM.get('SUB_DICT');
sub_list1 = sub_dict1.get('IT_LIST'); % Get all subjects as a cell array

% Use cellfun to create NNDataPoint_VOIs for each subject
it_list_voi1 = cellfun(@(sub) NNDataPoint_VOIs( ...
    'ID', sub.get('ID'), ...
    'VOI_DICT', IndexedDictionary( ...
        'ID', 'subject_idict', ...
        'IT_CLASS', 'SubjectNIfTI', ...
        'IT_KEY', IndexedDictionary.getPropDefault(IndexedDictionary.IT_KEY), ...
        'IT_LIST', sub.get('VOI_DICT').get('IT_LIST') ...
        ), ...
    'TARGET_CLASS', {group_folder_name1} ...
    ), sub_list1, 'UniformOutput', false);

% ǵroup 2
[~, group_folder_name2] = fileparts(im_gr2_PET.get('DIRECTORY'));
it_list2 = cellfun(@(x) NNDataPoint_Graph_CLA( ...
    'ID', x.get('ID'), ...
    'G', x, ...
    'TARGET_CLASS', {group_folder_name2}), ...
     a_WU2.get('G_DICT').get('IT_LIST'), ...
    'UniformOutput', false);

% Get the subject dictionary and extract the list of subjects
sub_dict2 = gr2_WM_GM.get('SUB_DICT');
sub_list2 = sub_dict2.get('IT_LIST'); % Get all subjects as a cell array

% Use cellfun to create NNDataPoint_VOIs for each subject
it_list_voi2 = cellfun(@(sub) NNDataPoint_VOIs( ...
    'ID', sub.get('ID'), ...
    'VOI_DICT', IndexedDictionary( ...
        'ID', 'subject_idict', ...
        'IT_CLASS', 'SubjectNIfTI', ...
        'IT_KEY', IndexedDictionary.getPropDefault(IndexedDictionary.IT_KEY), ...
        'IT_LIST', sub.get('VOI_DICT').get('IT_LIST') ...
        ), ...
    'TARGET_CLASS', {group_folder_name2} ...
    ), sub_list2, 'UniformOutput', false);

% ǵroup 3
[~, group_folder_name3] = fileparts(im_gr3_PET.get('DIRECTORY'));
it_list3 = cellfun(@(x) NNDataPoint_Graph_CLA( ...
    'ID', x.get('ID'), ...
    'G', x, ...
    'TARGET_CLASS', {group_folder_name3}), ...
     a_WU3.get('G_DICT').get('IT_LIST'), ...
    'UniformOutput', false);
% Get the subject dictionary and extract the list of subjects
sub_dict3 = gr3_WM_GM.get('SUB_DICT');
sub_list3 = sub_dict3.get('IT_LIST'); % Get all subjects as a cell array

% Use cellfun to create NNDataPoint_VOIs for each subject
it_list_voi3 = cellfun(@(sub) NNDataPoint_VOIs( ...
    'ID', sub.get('ID'), ...
    'VOI_DICT', IndexedDictionary( ...
        'ID', 'subject_idict', ...
        'IT_CLASS', 'SubjectNIfTI', ...
        'IT_KEY', IndexedDictionary.getPropDefault(IndexedDictionary.IT_KEY), ...
        'IT_LIST', sub.get('VOI_DICT').get('IT_LIST') ...
        ), ...
    'TARGET_CLASS', {group_folder_name3} ...
    ), sub_list3, 'UniformOutput', false);



dp_list1 = IndexedDictionary(...
        'IT_CLASS', 'NNDataPoint_Graph_CLA', ...
        'IT_LIST', it_list1 ...
        );

dp_list2 = IndexedDictionary(...
        'IT_CLASS', 'NNDataPoint_Graph_CLA', ...
        'IT_LIST', it_list2 ...
        );

dp_list3 = IndexedDictionary(...
        'IT_CLASS', 'NNDataPoint_Graph_CLA', ...
        'IT_LIST', it_list3 ...
        );


dp_list_voi1 = IndexedDictionary(...
        'IT_CLASS', 'NNDataPoint_VOIs', ...
        'IT_LIST', it_list_voi1 ...
        );

dp_list_voi2 = IndexedDictionary(...
        'IT_CLASS', 'NNDataPoint_VOIs', ...
        'IT_LIST', it_list_voi2 ...
        );

dp_list_voi3 = IndexedDictionary(...
        'IT_CLASS', 'NNDataPoint_VOIs', ...
        'IT_LIST', it_list_voi3 ...
        );



d1 = NNDataset( ...
    'DP_CLASS', 'NNDataPoint_Graph_CLA', ...
    'DP_DICT', dp_list1 ...
    );
d1_vois = NNDataset( ...
    'DP_CLASS', 'NNDataPoint_VOIs', ...
    'DP_DICT', dp_list_voi1 ...
    );

d2 = NNDataset( ...
    'DP_CLASS', 'NNDataPoint_Graph_CLA', ...
    'DP_DICT', dp_list2 ...
    );
d2_vois = NNDataset( ...
    'DP_CLASS', 'NNDataPoint_VOIs', ...
    'DP_DICT', dp_list_voi2 ...
    );

d3 = NNDataset( ...
    'DP_CLASS', 'NNDataPoint_Graph_CLA', ...
    'DP_DICT', dp_list3 ...
    );
d3_vois = NNDataset( ...
    'DP_CLASS', 'NNDataPoint_VOIs', ...
    'DP_DICT', dp_list_voi3 ...
    );

%% Create a classifier cross-validation
nn_template = NNClassifierMLP_VOIs('EPOCHS', 50, 'LAYERS', [128 128]);
num_dp_d1 = d1.get('DP_DICT').get('LENGTH'); % Number of data points in d1 (assumed same as d1_vois)
num_dp_d2 = d2.get('DP_DICT').get('LENGTH'); % Number of data points in d2 (assumed same as d2_vois)
% Generate shuffled split indices for 5 folds
shuffled_indices_d1 = randperm(num_dp_d1); % Random permutation of indices for d1
shuffled_indices_d2 = randperm(num_dp_d2); % Random permutation of indices for d2
% Calculate split points for 5 equal parts
split_points_d1 = round(linspace(0, num_dp_d1, 6)); % 6 points to define 5 segments
split_points_d2 = round(linspace(0, num_dp_d2, 6)); % 6 points to define 5 segments
SPLIT = cell(2, 5);
for i = 1:5
    SPLIT{1, i} = shuffled_indices_d1(split_points_d1(i)+1:split_points_d1(i+1));
    SPLIT{2, i} = shuffled_indices_d2(split_points_d2(i)+1:split_points_d2(i+1));
end
nncv = NNClassifierMLP_CrossValidation_VOIs('D', {d1, d2}, 'D_VOIS', {d1_vois, d2_vois}, 'KFOLDS', 5, 'NN_TEMPLATE', nn_template, 'SPLIT', SPLIT); % d2 healthy, d1 AD
nncv.get('TRAIN');



%% Evaluate the performance
confusion_matrix_ad = nncv.get('C_MATRIX');
av_auc_ad = nncv.get('AV_AUC');
av_macro_auc_ad = nncv.get('AV_MACRO_AUC');
sensitivity_ad = confusion_matrix_ad(1,1)/ sum(confusion_matrix_ad(:,1));
specificity_ad = confusion_matrix_ad(2,2)/ sum(confusion_matrix_ad(:,2));


%% Create a classifier cross-validation
nn_template = NNClassifierMLP_VOIs('EPOCHS', 50, 'LAYERS', [128 128]);
num_dp_d3 = d3.get('DP_DICT').get('LENGTH'); % Number of data points in d1 (assumed same as d1_vois)
num_dp_d2 = d2.get('DP_DICT').get('LENGTH'); % Number of data points in d2 (assumed same as d2_vois)
% Generate shuffled split indices for 5 folds
shuffled_indices_d3 = randperm(num_dp_d3); % Random permutation of indices for d1
shuffled_indices_d2 = randperm(num_dp_d2); % Random permutation of indices for d2
% Calculate split points for 5 equal parts
split_points_d3 = round(linspace(0, num_dp_d3, 6)); % 6 points to define 5 segments
split_points_d2 = round(linspace(0, num_dp_d2, 6)); % 6 points to define 5 segments
SPLIT = cell(2, 5);
for i = 1:5
    SPLIT{1, i} = shuffled_indices_d3(split_points_d3(i)+1:split_points_d3(i+1));
    SPLIT{2, i} = shuffled_indices_d2(split_points_d2(i)+1:split_points_d2(i+1));
end
nncv = NNClassifierMLP_CrossValidation_VOIs('D', {d3, d2},'D_VOIS', {d3_vois, d2_vois}, 'KFOLDS', 5, 'NN_TEMPLATE', nn_template, 'SPLIT', SPLIT);%d2 healthy, d3 MCI, d1 AD
nncv.get('TRAIN');

%% Evaluate the performance
confusion_matrix_mci = nncv.get('C_MATRIX');
av_auc_mci = nncv.get('AV_AUC');
av_macro_auc_mci = nncv.get('AV_MACRO_AUC');
specificity_mci  = confusion_matrix_mci(1,1)/ sum(confusion_matrix_mci(:,1));
sensitivity_mci = confusion_matrix_mci(2,2)/ sum(confusion_matrix_mci(:,2));

fprintf('Average AUC CN VS AD: %.4f\n', av_macro_auc_ad);
fprintf('Average Sensitivity AD: %.4f\n', sensitivity_ad);
fprintf('Average Specificity AD: %.4f\n', specificity_ad);

fprintf('Average AUC CN VS MCI: %.4f\n', av_macro_auc_mci);
fprintf('Average Sensitivity MCI: %.4f\n', sensitivity_mci);
fprintf('Average Specificity MCI: %.4f\n', specificity_mci);
%% add test on example data
% 
% num_subjects = a_WU1.get('G_DICT').get('LENGTH');
% for i = 1:num_subjects
%     g = a_WU1.get('G_DICT').get('IT', i);
%     strength = g.get('MEASURE', 'Strength').get('M'); % Strength for all regions
%     strength20_regions = strength{1}(1:20);
%     strengthother_regions = strength{1}(21:end);
%     mean_20 = mean(strength20_regions(:));
%     mean_others = mean(strengthother_regions(:));
%     % Assert for each subject
%     assert(mean_20 > mean_others, ...
%         sprintf('Test failed for subject %d: The first 20 regions do not have higher correlation than the other regions.', i));
% end
